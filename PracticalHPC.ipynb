{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Practical HPC</center>\n",
    "<p>\n",
    "## <center>Robert Bjornson</center> \n",
    "<p>\n",
    "## <center><i>Yale Center for Research Computing</i></center>\n",
    "<p>\n",
    "## <center>March 2018</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Installing Anaconda (includes Jupyter notebook)\n",
    "1. at www.continuum.io, download python 3.6 version of anaconda, and install\n",
    "\n",
    "## To get tutorial files\n",
    "1. browse to https://github.com/ycrc/PracticalHPC\n",
    "1. click \"clone or download\" then \"download zip\"\n",
    "1. unzip file into desired location\n",
    "\n",
    "## To follow presentation in Jupyter notebook\n",
    "1. in a terminal, cd to PracticalHPC\n",
    "1. run this command: jupyter notebook PracticalHPC.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the Yale Center for Research Computing?\n",
    "\n",
    "\n",
    "- Independent center under the Provost's office\n",
    "- Created to support your research computing needs\n",
    "- Focus is on high performance computing and storage\n",
    "- ~15 staff, including applications specialists and system engineers\n",
    "- Available to consult with and educate users\n",
    "- Manage compute clusters and support users\n",
    "- Located at 160 St. Ronan st, at the corner of Edwards and St. Ronan\n",
    "- http://research.computing.yale.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "- Understanding the HPC environment\n",
    "- Understanding your program's behavior\n",
    "- Improving performance\n",
    "- Running parallel programs\n",
    "- Creating parallel programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assumptions\n",
    "- Have logged in to a cluster and run simple jobs\n",
    "- understand basics of Slurm\n",
    "- want to run more effectively and efficiently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The HPC Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is HPC?\n",
    "- running jobs remotely on a compute cluster \n",
    "- linux-based\n",
    "- using more compute resources (cpus/memory) than you have locally\n",
    "- access to huge shared storage\n",
    "- access to specialized hardware, e.g. GPUs \n",
    "- access to specialized software\n",
    "- running large job in parallel, or many sequential jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![alt text](clusterschematic.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical Cluster\n",
    "\n",
    "![alt text](yalecluster.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Typical compute node\n",
    "![alt text](nx360-M5.jpg \"Title\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical Cluster (Yale, or elsewhere)\n",
    "- Several hundred servers (nodes)\n",
    "- Each node has 10s of cpus -> Thousands of total cpus\n",
    "- Each node has 100s of GB RAM\n",
    "- Multiple PB of shared storage\n",
    "- Fast network connecting nodes and networks\n",
    "- Shared by 100s of users, 1000s of jobs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster Resources\n",
    "These are managed by slurm, cgroups, and storage quotas\n",
    "- Nodes and CPUs\n",
    "- Node Memory (RAM)\n",
    "- Job Runtime\n",
    "- Cluster Disk Storage\n",
    "- GPUs\n",
    "\n",
    "### Note\n",
    "- jobs (unrelated) normally share nodes.  Slurm+cgroups makes sure you have\n",
    "exclusive access to your resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick Review of Slurm allocations\n",
    "\n",
    "$ sbatch [options] batch.sh\n",
    "\n",
    "$ srun --pty [options] bash\n",
    "\n",
    "option | example | comment\n",
    "-------|--------|---\n",
    "-p _partition_ | -p general | partitions(s) to run on\n",
    "-c _cpus_ | -c 20 | cpus/task on single node\n",
    "-n _tasks_ | -n 4 | mpi progs only\n",
    "-N _nodes_ | -N 10 | forces layout, rarely useful\n",
    "-t _time_  | -t 7-, -t 3:00 | job killed if exceeded\n",
    "--mem=_mem_ | --mem=16g | ditto\n",
    "--mem-per-cpu=_mem_ | --mem-per-cpu=8g | ditto\n",
    "-J _name_ | -J myjob | name job\n",
    "--gres | --gres=gpu:p100:2 | request gpus\n",
    "--array _spec_ | --array 1-10 | array job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fairshare Scheduling with backfill\n",
    "- Slurm tracks cpu usage by user and group\n",
    "- Job priority is determined by amount of recent usage\n",
    "- Users and Groups with heavy recent usage have lower priority\n",
    "- Backfill can run short, lower priority jobs\n",
    "- Bottom line: only ask for what you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nodes/CPUs\n",
    "- default: 1 node, 1 cpu\n",
    "- only request multiple cpus and/or nodes if your program can use them\n",
    "- simply requesting more nodes or cpus will not make things run faster!\n",
    "- cgroups pens you into your allocated cpus\n",
    "- understand -c (--cpus-per-task)\n",
    "     vs -n (--ntasks) vs -N (--nodes)\n",
    "- only request GPU or bigmem nodes if you need them\n",
    "- users have limits (e.g. 100 cpus)\n",
    "- -C _type_ (westmere, haswell, avx2, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Memory\n",
    "- default of 5 GB per allocated cpu on our clusters\n",
    "- strictly enforced; jobs exceeding limit are killed\n",
    "- users have limits (e.g. 640 GB)\n",
    "- you can request custom memory per node or core with sbatch or srun:\n",
    "```\n",
    "--mem=6g\n",
    "--mem-per-cpu=6g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding compute resources\n",
    "```\n",
    "sinfo -p general\n",
    "sinfo -p general -e -t IDLE -o \"%P %.5a %c %m %.10l %.6D %.6t %N\"\n",
    "alias findidle='sinfo -e -t IDLE -o \"%P %.5a %c %m %.10l %.6D %.6t %N\"'\n",
    "findidle -p general\n",
    "\n",
    "```\n",
    "## Graphical Tools\n",
    "\n",
    "http://overwatch.ycrc.yale.edu/farnam/orwell/\n",
    "\n",
    "http://overwatch.ycrc.yale.edu/grace/orwell/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scavenge Partition\n",
    "```\n",
    "$ sbatch -p scavenge\n",
    "```\n",
    "\n",
    "- Compute nodes in other partions are available via scavenge partition\n",
    "- jobs subject to being killed at any time\n",
    "- separate per user limits apply\n",
    "- works best for short jobs, dSQ/array jobs, or jobs that checkpoint\n",
    "- can include gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large Memory Nodes\n",
    "- Compute nodes with 512GB-1.5TB of RAM\n",
    "- Reserved for applications with large memory needs. Please be considerate.\n",
    "- Separate slurm partition: bigmem\n",
    "\n",
    "Typical allocation: \n",
    "```\n",
    "srun/sbatch -p bigmem --mem=1500g ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU Nodes\n",
    "- Some applications have been ported to GPUs with impressive performance improvement\n",
    "- Gpu nodes have conventional cpus with multiple cores, and 1-4 GPUs.  \n",
    "- To use GPUs, you must:\n",
    " - request node(s) with GPUs\n",
    " - request the type and number of GPUs \n",
    "\n",
    "Typical allocation:\n",
    "```\n",
    "srun --pty -p gpu -c 8 --gres=gpu:1080ti:4 ... bash\n",
    "sbatch -p gpu -c 8 --gres=gpu:1080ti:4 ... batch.sh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPU Nodes continued\n",
    "\n",
    "You can also scavenge gpu nodes:\n",
    "```\n",
    "srun --pty -p scavenge -c 8 --gres=gpu:1080ti:4 ... bash\n",
    "sbatch -p scavenge -c 8 --gres=gpu:1080ti:4 ... batch.sh\n",
    "```\n",
    "\n",
    "Note that partition names, types and number of GPUs vary by cluster.\n",
    "\n",
    "Type | FP | Clusters\n",
    "-------|--------|---\n",
    "1080ti | single | farnam\n",
    "K80 | double | grace,farnam\n",
    "P100 | double | grace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Your Job's Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is \"Running Efficiently\"?\n",
    "- All allocated cores are running near 100%\n",
    "- GPUs (if allocated) are used\n",
    "- memory requested is adequate and reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring your job\n",
    "```\n",
    "squeue -u netid -l (your jobs)\n",
    "squeue -j jobid -l (particular job)\n",
    "scontrol show job jobid (during job)\n",
    "sacct -j jobid -l (after job finishes)\n",
    "seff -j jobid     (after job finished)\n",
    "```\n",
    "Default output for squeue and sacct is not ideal.  Put in .bashrc:\n",
    "```\n",
    "export SACCT_FORMAT=\"JobID%-20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS, AllocTRES%32\"\n",
    "export SQUEUE_FORMAT=\"%.16i %.12P %.12j %.8u %.2t %.12M %.12l %24R %.4D %.4C %m %8b %8f\"\n",
    "```\n",
    "in PracticalHPC:\n",
    "\n",
    "```source SLURM.env```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking cpu and memory utilization for running jobs\n",
    "Using top/htop\n",
    "- use ```squeue -u netid``` to determine where your job is running\n",
    "- ssh to your nodes, especially 2nd and on, and run ```top -u netid``` \n",
    "    (or ```htop```)\n",
    "- look at %CPU and RES columns\n",
    "- should see ~%100 cpu for each allocated core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Postmortem checking\n",
    "\n",
    "1. Run program under /usr/bin/time\n",
    "```\n",
    "$ /usr/bin/time -a myprog arg1 arg2 ...\n",
    "```\n",
    "\n",
    "2. After run completes, determine actual usage (See format comment):\n",
    "\n",
    "```\n",
    "$ sacct -j jobid\n",
    "\n",
    "$ seff -j jobid\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Performance\n",
    "```\n",
    "cd Examples/Performance\n",
    "sbatch bwa.sh # runs out of RAM (default 5GB/core)\n",
    "sbatch --mem=10g bwa.sh # also runs out\n",
    "sbatch -c 8 # runs ok.  Show htop, scontrol, sacct, \n",
    "Htop: H combines threads.  F5 shows tree\n",
    "sacct -j jobid\n",
    "scontrol show job jobid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Storage Quotas\n",
    "- most quotas are group quotas\n",
    "- if exceeded, jobs will die trying to write\n",
    "- to see all quotas for your group:\n",
    "```\n",
    "$ groupquota \n",
    "$ groupquota -u netid\n",
    "```\n",
    "\n",
    "## File Storage\n",
    "\n",
    "Location | Purpose | Typical Limit (size/files)| Backed Up?\n",
    "-------|--------|---|----|\n",
    "home | scripts, programs, documents, small files | 125GB/500K | Y\n",
    "project | large data, programs | 1-4TB/5M | N \n",
    "scratch60 | temp data | 10TB/5M | purged\n",
    "pi\\__name_ | dedicated | _varies_ | N\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## File Transfer\n",
    "- For small transfers: \n",
    " - Linux/Mac/cygwin: scp, rsync\n",
    " - Windows: Mobaxterm, winscp\n",
    " \n",
    " \n",
    "- For larger transfers, or outside Yale, or collaborators without Yale credentials:\n",
    " - Globus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Globus\n",
    "- www.globus.org\n",
    "- web and command line\n",
    "- very fast and robust\n",
    "- transfers between _endpoints_\n",
    "- endpoints: \n",
    " - Yale clusters: yale#{cluster}\n",
    " - Lots of other institutions\n",
    " - personal endpoint on your own computer\n",
    "- you can create _shares_, accessible to non-Yale researchers via their globus id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"globus.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Example\n",
    "```\n",
    "show transfer between farnam and ruddle\n",
    "show personal endpoint (bjornson_mac)\n",
    "show creating share and sharing with jason.ignatius@yale.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Globus cli\n",
    "```\n",
    "$ module load Globus-CLI\n",
    "$ globus login\n",
    "(you'll get a url.  Open that in a browser, authenticate, and copy the code.  Enter the code on command line.)\n",
    "$ globus endpoint search bjornson_mac\n",
    "$ globus endpoint search yale#farnam\n",
    "(set env vars to BM and F)\n",
    "$ globus ls -l $BM:/\n",
    "$ globus transfer -r -s size $BM:Data $F:Data\n",
    "(returns taskid)\n",
    "$ globus task show taskid\n",
    "```\n",
    "https://docs.globus.org/cli/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance\n",
    "- When possible, use optimized libraries, rather than roll your own\n",
    "- Python: numpy or pandas instead of nested lists for matrices\n",
    "- R, Matlab: vectors instead of loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Profiling\n",
    "- Helps you pinpoint where time is spent\n",
    "\n",
    " - R: Rprof\n",
    " - Python: cProfile, kernprof/line_profiler\n",
    " - C: gprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python: cProfile for function level profiling\n",
    "```\n",
    "cd Examples/Profiling\n",
    "module load Python\n",
    "(get compute node with srun, module load Python)\n",
    "python -m cProfile -o bad.prof bad.py\n",
    "(takes a minute or two)\n",
    "\n",
    "(in python)\n",
    "import pstats\n",
    "p=pstats.Stats('bad.prof')\n",
    "p.sort_stats('time').print_stats()\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "\n",
    "## kernprof for line-by-line\n",
    "```\n",
    "module load Python\n",
    "(decorate functions with @profile)\n",
    "kernprof -l bad2.py\n",
    "python -m line_profiler  bad2.py.lprof\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "` ``\n",
    "rdb9@c13n08 Profiling]$ python -m line_profiler bad2.py.lprof\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 114.565 s\n",
    "File: bad.py\n",
    "Function: doit at line 4\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     4                                           @profile\n",
    "     5                                           def doit(gf, mf, of):\n",
    "     6         1          134    134.0      0.0      ofp=open(of, \"w\")\n",
    "     7         1            1      1.0      0.0      genenames=[]\n",
    "     8         1            0      0.0      0.0      geneinfo=[]\n",
    "     9     80923        44421      0.5      0.0      for l in open(gf):\n",
    "    10     80922       105965      1.3      0.1          gn, chrm, strand, start, end = l.strip().split()\n",
    "    11     80922        44241      0.5      0.0          genenames.append(gn)\n",
    "    12     80922        49721      0.6      0.0          geneinfo.append([chrm, strand, start, end])\n",
    "    13\n",
    "    14     71521        90557      1.3      0.1      for l in open(mf):\n",
    "    15     71520       126260      1.8      0.1          genename=l.strip().split()[-1]\n",
    "    16\n",
    "    17     71520    110808166   1549.3     96.7          idx=genenames.index(genename)\n",
    "    18     71520      3295037     46.1      2.9          print(l + str(geneinfo[idx]), file=ofp)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## FIX: replace list with dictionary\n",
    "```\n",
    "kernprof -l good.py > /dev/null\n",
    "python -m line_profiler good.py.lprof\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelism\n",
    "\n",
    "- Sbatch can allocate multiple cores and nodes, but the script runs on one core on one node sequentially.\n",
    "\n",
    "- _Simply allocating more nodes or cores DOES NOT make jobs faster._\n",
    "\n",
    "- How do we use multiple cores to increase speed?\n",
    "\n",
    "\n",
    "- Two classes of parallelism:\n",
    " - Lots of independent sequential jobs\n",
    " - Single job parallelized (somehow, maybe by someone else?)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First, my \"Disclaimer\"\n",
    "- Parallel computing is a complex field with many tools and techniques\n",
    "- Entire courses: e.g. Yale CS 424/524 \"Parallel Programming Techniques\" by Dr. Andrew Sherman\n",
    "- We'll see some simple techniques for easy cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slurm Job Arrays\n",
    "\n",
    "- Useful when you have many nearly identical, independent jobs to run\n",
    "- Starts many identical copies of your script, distinguished by $SLURM_ARRAY_TASK_ID\n",
    "\n",
    "Submit jobs like this:\n",
    "```\n",
    "sbatch --array=1-100 script.sh\n",
    "```\n",
    "Inside your batch script this environment variable to do something different in each task:\n",
    "```\n",
    "./mycommand -i input.${SLURM_ARRAY_TASK_ID} \\\n",
    "    -o output.${SLURM_ARRAY_TASK_ID}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slurm Job Arrays continued\n",
    "A few nice features of job arrays:\n",
    "- only one job to keep track of\n",
    "- easy to start or cancel entire set\n",
    "- slurm options, e.g. cpus, memory, time limits, apply to each task, not overall job\n",
    "- your allocation can grow and shrink as conditions change\n",
    "- when using scavenge partition, tasks are killed, but job persists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Array job example\n",
    "```\n",
    "cd Examples/Array\n",
    "sbatch array.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## dSQ (aka Dead Simple Queue)\n",
    "- local tool built on job arrays.  Same nice features, but easier to use\n",
    "- more flexible; tasks can be arbitrarily different \n",
    "- reporting and error recovery built in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using dSQ\n",
    "\n",
    "\n",
    "- Create file containing list of commands to run (jobs.txt)\n",
    "```\n",
    "prog arg1 arg2 -o job1.out\n",
    "prog arg1 arg2 -o job2.out\n",
    "...\n",
    "```\n",
    "- Create batch script\n",
    "```\n",
    "module load dSQ\n",
    "dSQ --jobfile jobs.txt [slurm args] > run.sh\n",
    "```\n",
    "\n",
    "slurm args can specify partion, timelimit, memory, etc. in the usual way.\n",
    "\n",
    "- Submit launch script\n",
    "```\n",
    "sbatch run.sh\n",
    "```\n",
    "\n",
    "For more info, see <http://research.computing.yale.edu/support/hpc/user-guide/dead-simple-queue>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# dSQ Reporting\n",
    "- When dSQ job is finished, you'll see a file `job_<jobid>_status.tsv`\n",
    "- Generate report:\n",
    "```\n",
    "$ dSQAutopsy jobs.txt job_<jobid>_status.tsv > failedjobs.txt\n",
    "Autopsy Task Report:\n",
    "9 succeeded\n",
    "1 failed\n",
    "0 didn't run.\n",
    "```\n",
    "\n",
    "- If any jobs failed, failedjobs.txt will contain those jobs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## dSQ Example\n",
    "```\n",
    "cd Examples/GenomePL\n",
    "python mktasks.py data output > tasks.sh # create dSQ job file\n",
    "module load dSQ\n",
    "dSQ --jobfile tasks.sh > run.sh\n",
    "sbatch run.sh # will fail, do sacct -j jobid, resubmit --mem=10g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Running already parallelized applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel bootstrap in R using Multicore library\n",
    "- works on multiple cpus on one node\n",
    "\n",
    "```\n",
    "boot(data=trees, statistic=volume_estimate, R=50000, parallel=\"multicore\", ncpus=8)\n",
    "\n",
    "\n",
    "cd Examples/R-Bootstrap\n",
    "sbatch -c 8 ... script.sh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Namd\n",
    "- molecular dynamics simulation written in C and C++\n",
    "- Namd is parallelized several ways, which can be combined:\n",
    " - OpenMP (within one node)\n",
    " - MPI (across nodes)\n",
    " - GPU\n",
    " \n",
    "Example: Satellite Tobacco Mosaic Virus 1,066,628 atoms, 500 time steps\n",
    "\n",
    "\n",
    "![alt text](Examples\\Namd\\namd.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Namd Performance on Grace\n",
    "\n",
    "   |startup (s)|simulate|total|step|step s/u\n",
    "---|-------|--------|-----|----|----\n",
    "1 node, 1 cpu|14|5833|5847|11.6|1.0\n",
    "1 node, 28 cpu|13|218|231|0.44|26.3\n",
    "1 node, 20 cpu+4 K80 gpus|12|32|44|.056|207\n",
    "9 nodes, 40 cpus|29.5|212|242|.424|27.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "in /home/fas/lsprog/rdb9/repos/PracticalHPC/Examples/Namd/stmv\n",
    "- gpus slurm-9139008.out\n",
    "- mpi slurm-9152709.out\n",
    "- 28 core MP slurm-9139007.out\n",
    "- 1 core slurm-9140416.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Namd Example\n",
    "```\n",
    "cd Examples/Namd/stmv\n",
    "sbatch multi.sh\n",
    "sbatch gpu.sh\n",
    "use htop and nvidia-smi on allocated node\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DIY Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Kmeans in R using parallel\n",
    "\n",
    "- simple idea: reduce computation to applying function to list of values (very R-ish)\n",
    "- then do that in parallel\n",
    "\n",
    "sequential\n",
    "```\n",
    "starts=100000; tasks=8; nstarts=rep(starts/tasks, tasks)\n",
    "results<-lapply(nstarts, function(nstart) kmeans(Boston, 4, nstart=nstart)))\n",
    "```\n",
    "\n",
    "parallel\n",
    "```\n",
    "library(parallel)\n",
    "...\n",
    "results<-mclapply(nstarts, \n",
    "  function(nstart) kmeans(Boston, 4, nstart=nstart), \n",
    "  mc.cores=cores)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Kmeans\n",
    "```\n",
    "cd R-Kmeans\n",
    "sbatch kmeans.sh\n",
    "sbatch parkmeans.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## kmeans performance on farnam m610s\n",
    "\n",
    "   |runtime|s/u\n",
    "---|-------|-------\n",
    "1 node, 1 cpu| 44.475 | -\n",
    "1 node, 8 cpus | 5.96 | 7.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other thoughts on parallel R\n",
    "- _Parallel R_, by Steve Weston, O'Reilly Press\n",
    "- YCRC Bootcamp: Writing Efficient R Code (taught by Steve)\n",
    "- ```foreach``` parallel construct (written by Steve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Python Multiprocessing\n",
    "- Support for creating parallel processes and communicating and synchronizing\n",
    "- Simplest interface is parallel map, very similar to R's mclapply\n",
    "\n",
    "```\n",
    "import multiprocessing\n",
    "p=multiprocessing.Pool(10)\n",
    "inputs=[...]\n",
    "results=p.map(f, inputs)\n",
    "```\n",
    "\n",
    "- Pool creates set of \"workers\" that compute f() on inputs\n",
    "- Workers are forked processes.  Have copies of data at point where pool was created\n",
    "- Function, inputs, and outputs are serialized and passed between master and workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Travelling salesman in Python using multiprocessing\n",
    "- Given N cities, find shortest route visiting each once\n",
    "- NP-complete -> very expensive to find optimal solution\n",
    "- We'll use a heuristic approach, simulated annealing:\n",
    "\n",
    "```\n",
    "T = initialT\n",
    "curr = best = initialsolution\n",
    "while T > stopT:\n",
    "    candidate = randomly swap two adjacent cities in curr\n",
    "    if cost(candidate) < cost(best):\n",
    "        curr=candidate\n",
    "        best=candidate\n",
    "    elif cost(candidate)-cost(best) < T\n",
    "        curr=candidate\n",
    "    T = T * alpha\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallelization strategy\n",
    "- Run K copies, starting from different initial conditions\n",
    "- Take different random paths\n",
    "- Choose the best solution\n",
    "\n",
    "Example/simulated-annealing-tsp\n",
    "\n",
    "Steps:\n",
    "- seq.py: explicit for loop over simulations\n",
    "- map.py: convert loop to map\n",
    "- parmap.py: convert map to parallel map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## parmap.py\n",
    "\n",
    "```\n",
    "def dotrial(t):\n",
    "    t.anneal()\n",
    "    return t\n",
    "\n",
    "p=multiprocessing.Pool(cores) \n",
    "trials = [SimAnneal(coords, ...) for i in range(ntrials)]\n",
    "got=p.map(dotrial, trials, chunksize=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: TSP\n",
    "```\n",
    "cd Examples/TSP\n",
    "srun -p interactive --pty bash\n",
    "python seq.py 10\n",
    "python map.py 10\n",
    "srun -c 10 -p interactive --pty bash\n",
    "python parmap.py 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Subtleties WRT random numbers\n",
    "- When running in parallel, what random number sequence is seen by each process?\n",
    "- many random number generators use system clock, which may not be different for each process\n",
    "- you may want reproduceable results\n",
    "\n",
    "- R: use RNGkind(\"L'Ecuyer-CMRG\").  Sets up independent, reproducible random streams\n",
    "- Matlab and Python: Each worker has same seed by default.  You should explicitly set it, e.g. rng(workerid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C/C++: OpenMP\n",
    "- popular way to parallelize C programs on single node\n",
    "- annotate for loop with #pragma omp for \n",
    "\n",
    "```\n",
    "#pragma omp for reduction(+:sum)\n",
    "  for (i=0; i<len*threads; i++)\n",
    "    {\n",
    "      sum += (a[i] * b[i]);\n",
    "      psum = sum;\n",
    "    }\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Matrix multiply with OpenMP\n",
    "```\n",
    "cd Examples/OpenMP\n",
    "make mm\n",
    "sbatch mm.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To get help or report problems\n",
    "- Check our status page: http://research.computing.yale.edu/system-status\n",
    "- Send an email to our tracking system: hpc@yale.edu\n",
    "- Read documentation: http://research.computing.yale.edu/hpc-support\n",
    "- Office hours: http://research.computing.yale.edu/hpc-support/office-hours-support\n",
    "- Email me directly: Robert.bjornson@yale.edu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
